{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import json\n",
    "import slack\n",
    "from datetime import datetime\n",
    "from simple_salesforce import Salesforce, SalesforceLogin, SFType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Salesforce ###\n",
    "logininfo = json.load(open('/ghds/groups/labdesk/bshih/salesforce_login.json'))\n",
    "\n",
    "username = logininfo['username']\n",
    "password = logininfo['password']\n",
    "security_token = logininfo['security_token']\n",
    "domain = 'login'\n",
    "\n",
    "sf = Salesforce(username=username, password=password, security_token=security_token)\n",
    "\n",
    "### Slack ###\n",
    "slack_token = json.load(open('/ghds/groups/labdesk/bshih/slack_login.json'))['SLACK_TOKEN']\n",
    "client = slack.WebClient(token=slack_token)\n",
    "\n",
    "# client.chat_postMessage(channel='#bot_testing', text='CAN I GET A OOOOO YEAAA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_finder(all_samples):\n",
    "    samples = all_samples[all_samples['run_sample_id'].str.startswith(('G', 'H'), na=False)]\n",
    "    samples = samples[~samples['run_sample_id'].str.startswith('Ht', na=False)]\n",
    "    filtered = samples.groupby('run_sample_id').filter(lambda x: (x['rnase_count'].median() <= 50) & (x['covid_ratio'].median() >= 0.01))\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def salesforce_query(samples):\n",
    "    weird_samples = []\n",
    "    for i in samples['run_sample_id'].unique():\n",
    "        weird_samples.extend(sf.query_all(f\"SELECT GH_Sample_ID__c, Status, Specimen_Collection_Date_Time__c, Specimen_receipt_date__c,\\\n",
    "                  State_Authorities_Notified_Date__c, Site_Name__c \\\n",
    "                  FROM Order WHERE GH_Sample_ID__c = '{i}'\").get('records'))\n",
    "\n",
    "    dataframe = pd.DataFrame(weird_samples)\n",
    "    df = dataframe.drop(columns='attributes').rename(columns={'GH_Sample_ID__c':'GH Sample ID',\n",
    "                                                       'Specimen_Collection_Date_Time__c':'Specimen Collection Date/Time',\n",
    "                                                       'Specimen_receipt_date__c':'Specimen receipt date',\n",
    "                                                       'State_Authorities_Notified_Date__c':'State Authorities Notified Date',\n",
    "                                                       'Site_Name__c':'Site Name'})\n",
    "    df['Specimen Collection Date/Time'] = pd.to_datetime(df['Specimen Collection Date/Time']).dt\\\n",
    "                                            .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "    df['State Authorities Notified Date'] = pd.to_datetime(df['State Authorities Notified Date']).dt\\\n",
    "                                            .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "    df['Specimen receipt date'] = pd.to_datetime(df['Specimen receipt date']).dt.strftime('%-m/%-d/%y')\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_sample_salesforce(samples, salesforce_data):\n",
    "    df = pd.merge(samples, salesforce_data, left_on='run_sample_id', right_on='GH Sample ID', how='left').loc[:,['runid',\n",
    "                                                                                                                'run_sample_id', \n",
    "                                                                                                                'covid_ratio', \n",
    "                                                                                                                'covid_count', \n",
    "                                                                                                                'rnase_count', \n",
    "                                                                                                                'spikein_count', \n",
    "                                                                                                                'replicate_call', \n",
    "                                                                                                                'replicate_flags', \n",
    "                                                                                                                'Specimen Collection Date/Time', \n",
    "                                                                                                                'Specimen receipt date', \n",
    "                                                                                                                'State Authorities Notified Date', \n",
    "                                                                                                                'Site Name']]\\\n",
    "                                                                                                            .sort_values(by=['runid', 'run_sample_id', 'covid_ratio'], ascending=[False, True, True])\n",
    "    \n",
    "    median_scores = df.groupby('run_sample_id').median().rename(columns={'covid_ratio':'new_median'})[['new_median']].round(2)\n",
    "    df = pd.merge(df, median_scores, left_on='run_sample_id', right_on='run_sample_id', how='left')\n",
    "    median = df.pop('new_median')\n",
    "    df.insert(2, 'new_median', median)\n",
    "    \n",
    "    df['group'] = (df['run_sample_id'].shift() != df['run_sample_id']).cumsum()\n",
    "    df.loc[df.duplicated('group'), ['runid', 'run_sample_id', 'new_median', 'Specimen Collection Date/Time', 'Specimen receipt date', 'State Authorities Notified Date', 'Site Name', 'new_median']] = ''\n",
    "    df.drop(columns=['group'], inplace=True)\n",
    "    \n",
    "    df = df.astype({'covid_count':'int', 'rnase_count':'int', 'spikein_count':'int'}).round({'covid_ratio':2})\n",
    "    return df\n",
    "    \n",
    "def create_pdf(samples, file_name):\n",
    "    fig, ax =plt.subplots(figsize=(35,len(samples)/2))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    the_table = ax.table(cellText=samples.values,colLabels=samples.columns,loc='center', rowLoc='right',bbox=[0, 0, 1, 1])\n",
    "\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(12)\n",
    "\n",
    "    for i in range(0, len(blah.columns)):\n",
    "        [the_table[(j+k, i)].set_facecolor(\"#e0e0e0\") for j in range(4, len(blah), 6) for k in range(3)]\n",
    "\n",
    "    the_table.auto_set_column_width(col=list(range(len(samples.columns))))\n",
    "\n",
    "    pp = PdfPages(f\"/ghds/groups/labdesk/bshih/{file_name}.pdf\")\n",
    "    pp.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "    pp.close()\n",
    "    \n",
    "def slack_upload(fcid, file_name):\n",
    "    filepath = f'/ghds/groups/labdesk/bshih/{file_name}.pdf'\n",
    "\n",
    "    try:\n",
    "        result = client.files_upload(\n",
    "            channels='#g19_sample_observation',\n",
    "            file=filepath,\n",
    "            initial_comment=f\"{fcid}: These samples have RNAse Count <= 50 and G19 Score >= 0.01\")\n",
    "        assert result[\"file\"]\n",
    "        print('File Successfully Uploaded to Slack!')\n",
    "    except SlackApiError as e:\n",
    "        # You will get a SlackApiError if \"ok\" is False\n",
    "        assert e.response[\"ok\"] is False\n",
    "        assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "        print(f\"Got an error: {e.response['error']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples_singlicate = pd.read_pickle('/ghds/groups/labdesk/bshih/c19dash/c19_dashboard/c19_read_counts.pickle')\n",
    "all_samples = sample_finder(all_samples_singlicate)\n",
    "salesforce_data = salesforce_query(all_samples)\n",
    "blah = merge_sample_salesforce(all_samples, salesforce_data)\n",
    "# create_pdf(blah, 'samples_test')\n",
    "# slack_upload('fcid', 'samples_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.to_csv('/ghds/groups/labdesk/bshih/clinopstest/210103_NB552478_0010_AHJ3KKBGXG_alltrue/c19_read_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples_singlicate.head(50).to_csv('/ghds/groups/labdesk/bshih/clinopstest/210103_NB552478_0010_AHJ3KKBGXG_allfalse/c19_read_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import watchdog.events\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from simple_salesforce import Salesforce\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from watchdog.observers.polling import PollingObserver as Observer\n",
    "\n",
    "### Salesforce API Login ###\n",
    "logininfo = json.load(open('/ghds/groups/labdesk/bshih/salesforce_login.json'))\n",
    "\n",
    "username = logininfo['username']\n",
    "password = logininfo['password']\n",
    "security_token = logininfo['security_token']\n",
    "domain = 'login'\n",
    "\n",
    "sf = Salesforce(username=username, password=password, security_token=security_token)\n",
    "\n",
    "### Slack Token ###\n",
    "slack_token = json.load(open('/ghds/groups/labdesk/bshih/slack_login.json'))['SLACK_TOKEN']\n",
    "client = WebClient(token=slack_token)\n",
    "\n",
    "### Logger Setup ###\n",
    "# LOGGER_NAME = 'clinical_observation2'\n",
    "# LOG_FILE_DIR = '/ghds/groups/labdesk/bshih'\n",
    "LOGGER_NAME = 'clinical_observation_demonstration'\n",
    "LOG_FILE_DIR = '/home/bshih/'\n",
    "\n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "# log_file = f\"{LOG_FILE_DIR}/clinical_observation2.log\"\n",
    "log_file = f\"{LOG_FILE_DIR}/clinical_observation_demonstration.log\"\n",
    "handler = logging.FileHandler(log_file)\n",
    "\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "class Handler(watchdog.events.FileSystemEventHandler):\n",
    "    def __init__(self):\n",
    "        watchdog.events.FileSystemEventHandler()\n",
    "        logger.info(f'Program has been initiated!')\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_finder(all_samples):\n",
    "        \"\"\"\n",
    "        :param all_samples:\n",
    "        :return: Filtered dataframe with samples whose RNAse <= 50 and G19 >= 0.01, singlicates.\n",
    "        \"\"\"\n",
    "        samples = all_samples[all_samples['run_sample_id'].str.startswith(('G', 'H'), na=False)]\n",
    "        samples = samples[~samples['run_sample_id'].str.startswith('Ht', na=False)]\n",
    "        filtered = samples.groupby('run_sample_id').filter(\n",
    "            lambda x: (x['rnase_count'].median() <= 50) & (x['covid_ratio'].median() >= 0.01))\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def salesforce_query(samples):\n",
    "        \"\"\"\n",
    "        :param samples:\n",
    "        :return: Salesforce query of filtered samples\n",
    "        \"\"\"\n",
    "        weird_samples = []\n",
    "        for i in samples['run_sample_id'].unique():\n",
    "            weird_samples.extend(sf.query_all(f\"SELECT GH_Sample_ID__c, Status, Specimen_Collection_Date_Time__c, Specimen_receipt_date__c,\\\n",
    "                      State_Authorities_Notified_Date__c, Site_Name__c \\\n",
    "                      FROM Order WHERE GH_Sample_ID__c = '{i}'\").get('records'))\n",
    "\n",
    "        dataframe = pd.DataFrame(weird_samples)\n",
    "        df = dataframe.drop(columns='attributes').rename(columns={'GH_Sample_ID__c': 'GH Sample ID',\n",
    "                                                                  'Specimen_Collection_Date_Time__c': 'Specimen Collection Date/Time',\n",
    "                                                                  'Specimen_receipt_date__c': 'Specimen receipt date',\n",
    "                                                                  'State_Authorities_Notified_Date__c': 'State Authorities Notified Date',\n",
    "                                                                  'Site_Name__c': 'Site Name'})\n",
    "        df['Specimen Collection Date/Time'] = pd.to_datetime(df['Specimen Collection Date/Time']).dt \\\n",
    "            .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "        df['State Authorities Notified Date'] = pd.to_datetime(df['State Authorities Notified Date']).dt \\\n",
    "            .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "        df['Specimen receipt date'] = pd.to_datetime(df['Specimen receipt date']).dt.strftime('%-m/%-d/%y')\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_sample_salesforce(samples, salesforce_data):\n",
    "        \"\"\"\n",
    "        :param samples:\n",
    "        :param salesforce_data:\n",
    "        :return: Merged dataframe samples with salesforce data\n",
    "        \"\"\"\n",
    "        df = pd.merge(samples, salesforce_data, left_on='run_sample_id', right_on='GH Sample ID', how='left').loc[:,\n",
    "             ['runid',\n",
    "              'run_sample_id',\n",
    "              'covid_ratio',\n",
    "              'covid_count',\n",
    "              'rnase_count',\n",
    "              'spikein_count',\n",
    "              'replicate_call',\n",
    "              'replicate_flags',\n",
    "              'Specimen Collection Date/Time',\n",
    "              'Specimen receipt date',\n",
    "              'State Authorities Notified Date',\n",
    "              'Site Name']] \\\n",
    "            .sort_values(by=['runid', 'run_sample_id', 'covid_ratio'], ascending=[False, True, True])\n",
    "\n",
    "        median_scores = df.groupby('run_sample_id').median().rename(columns={'covid_ratio': 'new_median'})[\n",
    "            ['new_median']].round(2)\n",
    "        df = pd.merge(df, median_scores, left_on='run_sample_id', right_on='run_sample_id', how='left')\n",
    "        median = df.pop('new_median')\n",
    "        df.insert(2, 'new_median', median)\n",
    "\n",
    "        df['group'] = (df['run_sample_id'].shift() != df['run_sample_id']).cumsum()\n",
    "        df.loc[df.duplicated('group'), ['runid', 'run_sample_id', 'new_median', 'Specimen Collection Date/Time',\n",
    "                                        'Specimen receipt date', 'State Authorities Notified Date', 'Site Name',\n",
    "                                        'new_median']] = ''\n",
    "        df.drop(columns=['group'], inplace=True)\n",
    "\n",
    "        df = df.astype({'covid_count': 'int', 'rnase_count': 'int', 'spikein_count': 'int'}).round({'covid_ratio': 2})\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pdf(samples, file_name):\n",
    "        fig, ax = plt.subplots(figsize=(35, len(samples) // 2))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        the_table = ax.table(cellText=samples.values, colLabels=samples.columns, loc='center', rowLoc='right',\n",
    "                             bbox=[0, 0, 1, 1])\n",
    "\n",
    "        the_table.auto_set_font_size(False)\n",
    "        the_table.set_fontsize(12)\n",
    "\n",
    "        for i in range(0, len(samples.columns)):\n",
    "            [the_table[(j + k, i)].set_facecolor(\"#e0e0e0\") for j in range(4, len(samples), 6) for k in range(3)]\n",
    "\n",
    "        the_table.auto_set_column_width(col=list(range(len(samples.columns))))\n",
    "\n",
    "        pp = PdfPages(f\"/ghds/groups/labdesk/bshih/{file_name}.pdf\")\n",
    "        pp.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "        pp.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def slack_upload(fcid, file_name):\n",
    "        filepath = f'/ghds/groups/labdesk/bshih/{file_name}.pdf'\n",
    "\n",
    "        try:\n",
    "            result = client.files_upload(\n",
    "                channels='#g19_sample_observation',\n",
    "                file=filepath,\n",
    "                initial_comment=f\"{fcid}: These samples have RNAse Count <= 50 and G19 Score >= 0.01\")\n",
    "            assert result[\"file\"]\n",
    "            logger.info('File Successfully Uploaded to Slack!')\n",
    "        except SlackApiError as e:\n",
    "            # You will get a SlackApiError if \"ok\" is False\n",
    "            assert e.response[\"ok\"] is False\n",
    "            assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "            logger.info(f\"Got an error: {e.response['error']}\")\n",
    "\n",
    "    def on_any_event(self, event):\n",
    "        logger.info(f'This file has been changed. Source path --> {event.src_path}')\n",
    "\n",
    "        today = datetime.strftime(datetime.today(), '%y%m%d')\n",
    "#         if bool(re.match(f'^.*?{today}.*?c19_read_counts\\.hdr\\.tsv$', event.src_path)):\n",
    "        if bool(re.match('^.*210103.*c19_read_counts\\.csv?', event.src_path)):\n",
    "\n",
    "#             flowcell = event.src_path[20:51]\n",
    "            flowcell = event.src_path[39:70]\n",
    "\n",
    "            logger.info(f'Flowcell {flowcell} c19_read_counts created!')\n",
    "\n",
    "#             current_read_counts = pd.read_csv(event.src_path, sep='\\t')\n",
    "            current_read_counts = pd.read_csv(event.src_path)\n",
    "            low_rnase_high_covid = Handler.sample_finder(current_read_counts)\n",
    "\n",
    "            if len(low_rnase_high_covid) != 0:\n",
    "                logger.info(f'There are anomaly samples in: {flowcell}')\n",
    "\n",
    "                salesforce_data = Handler.salesforce_query(low_rnase_high_covid)\n",
    "                merged_data = Handler.merge_sample_salesforce(low_rnase_high_covid, salesforce_data)\n",
    "\n",
    "                filename = flowcell + '_lowRNAse_COVIDpositive'\n",
    "\n",
    "                Handler.create_pdf(merged_data, filename)\n",
    "                Handler.slack_upload(flowcell, filename)\n",
    "\n",
    "                logger.info(f'Salesforce API Usage: {sf.api_usage}')\n",
    "\n",
    "            else:\n",
    "                client.chat_postMessage(channel='#g19_sample_observation',\n",
    "                                        text=f'{flowcell}: No QC fail samples that are covid positive today! Have a great day :)')\n",
    "                logger.info('No QC fail samples that are covid positive today! Have a great day :)')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     src_path = \"/ghds/cv19/analysis\"\n",
    "    src_path = \"/ghds/groups/labdesk/bshih/\"\n",
    "    event_handler = Handler()\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path=src_path, recursive=True)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            logger.info(\"Program is running every 1 Hour!\")\n",
    "            time.sleep(3600)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "        observer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/ghds/cv19/analysis/210104_NB552478_0010_AHJ3KKBGXG.02ab0e87-1aa3-44ea-8ea6-a1f1e1eb7447.20210101144150/c19_read_counts.hdr.tsv'\n",
    "today = datetime.strftime(datetime.today(), '%y%m%d')\n",
    "\n",
    "if bool(re.match(r'^.*?210104.*?c19_read_counts\\.hdr.tsv$', path)):\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "LOGGER_NAME = 'clinical_observation'\n",
    "LOG_FILE_DIR = '/ghds/groups/labdesk/bshih'\n",
    "    \n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "log_file = f\"{LOG_FILE_DIR}/clinical_observation.log\"\n",
    "handler = logging.FileHandler(log_file)\n",
    "\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def main():\n",
    "    logger.info(f\"Program is running!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
