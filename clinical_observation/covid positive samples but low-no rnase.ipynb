{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import watchdog.events\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from simple_salesforce import Salesforce\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from watchdog.observers.polling import PollingObserver as Observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel='#testing'\n",
    "\n",
    "### Salesforce ###\n",
    "def sf_login():\n",
    "    logininfo = json.load(open('/ghds/groups/labdesk/bshih/salesforce_login.json'))\n",
    "\n",
    "    username = logininfo['username']\n",
    "    password = logininfo['password']\n",
    "    security_token = logininfo['security_token']\n",
    "\n",
    "    return Salesforce(username=username, password=password, security_token=security_token)\n",
    "\n",
    "### Slack ###\n",
    "def slack_login():\n",
    "    slack_token = json.load(open('/ghds/groups/labdesk/bshih/slack_login.json'))['SLACK_TOKEN']\n",
    "    return WebClient(token=slack_token)\n",
    "\n",
    "def sample_finder(all_samples):\n",
    "    \"\"\"\n",
    "    :param all_samples:\n",
    "    :return: Filtered dataframe with samples whose RNAse <= 50 and G19 >= 0.01, singlicates.\n",
    "    \"\"\"\n",
    "    samples = all_samples[all_samples['run_sample_id'].str.startswith(('G', 'H'), na=False)]\n",
    "    samples = samples[~samples['run_sample_id'].str.startswith('Ht', na=False)]\n",
    "    filtered = samples.groupby('run_sample_id').filter(\n",
    "        lambda x: (x['rnase_count'].median() <= 50) & (x['covid_ratio'].median() >= 0.01))\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def salesforce_query(samples, sf):\n",
    "    weird_samples = []\n",
    "    accession = []\n",
    "    for i in samples['run_sample_id'].unique():\n",
    "        weird_samples.extend(sf.query_all(f\"SELECT GH_Sample_ID__c, Status, Specimen_Collection_Date_Time__c, Specimen_receipt_date__c,\\\n",
    "                  State_Authorities_Notified_Date__c, Site_Name__c \\\n",
    "                  FROM Order WHERE GH_Sample_ID__c = '{i}'\").get('records'))\n",
    "\n",
    "        accession.extend(sf.query_all(f\"SELECT Specimen_Id__c, Accessioning_Verified_Date__c FROM Specimen__c\\\n",
    "                                        WHERE Specimen_Id__c = '{i}'\").get('records'))\n",
    "\n",
    "    dataframe = pd.DataFrame(weird_samples)\n",
    "    df = dataframe.drop(columns='attributes').rename(columns={'GH_Sample_ID__c': 'run_sample_id',\n",
    "                                                              'Specimen_Collection_Date_Time__c': 'Specimen Collection Date/Time',\n",
    "                                                              'State_Authorities_Notified_Date__c': 'State Authorities Notified Date',\n",
    "                                                              'Site_Name__c': 'Site Name'})\n",
    "    df['Specimen Collection Date/Time'] = pd.to_datetime(df['Specimen Collection Date/Time']).dt \\\n",
    "        .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "    try:\n",
    "        df['State Authorities Notified Date'] = pd.to_datetime(df['State Authorities Notified Date']).dt \\\n",
    "            .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    dataframe2 = pd.DataFrame(accession)\n",
    "    df2 = dataframe2.drop(columns='attributes').rename(columns={'Specimen_Id__c': 'run_sample_id',\n",
    "                                                                'Accessioning_Verified_Date__c': 'accession_date'})\n",
    "\n",
    "    df2['accession_date'] = pd.to_datetime(df2['accession_date']).dt \\\n",
    "        .tz_convert('US/Pacific').dt.strftime('%-m/%-d/%Y, %-I:%M %p')\n",
    "\n",
    "    df = df.merge(df2, left_on='run_sample_id', right_on='run_sample_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_sample_salesforce(samples, salesforce_data):\n",
    "    \"\"\"\n",
    "    :param samples:\n",
    "    :param salesforce_data:\n",
    "    :return: Merged dataframe samples with salesforce data\n",
    "    \"\"\"\n",
    "    df = pd.merge(samples, salesforce_data, left_on='run_sample_id', right_on='run_sample_id', how='left').loc[:,\n",
    "         ['runid',\n",
    "          'run_sample_id',\n",
    "          'covid_ratio',\n",
    "          'covid_count',\n",
    "          'rnase_count',\n",
    "          'spikein_count',\n",
    "          'replicate_call',\n",
    "          'replicate_flags',\n",
    "          'Specimen Collection Date/Time',\n",
    "          'accession_date',\n",
    "          'State Authorities Notified Date',\n",
    "          'Site Name']] \\\n",
    "        .sort_values(by=['runid', 'run_sample_id', 'covid_ratio'], ascending=[False, True, True])\n",
    "\n",
    "    median_scores = df.groupby('run_sample_id').median().rename(columns={'covid_ratio': 'new_median'})[\n",
    "        ['new_median']].round(2)\n",
    "    df = pd.merge(df, median_scores, left_on='run_sample_id', right_on='run_sample_id', how='left')\n",
    "    median = df.pop('new_median')\n",
    "    df.insert(2, 'new_median', median)\n",
    "\n",
    "    df['group'] = (df['run_sample_id'].shift() != df['run_sample_id']).cumsum()\n",
    "    df.loc[df.duplicated('group'), ['runid', 'run_sample_id', 'new_median', 'Specimen Collection Date/Time',\n",
    "                                    'accession_date', 'State Authorities Notified Date', 'Site Name',\n",
    "                                    'new_median']] = ''\n",
    "    df.drop(columns=['group'], inplace=True)\n",
    "\n",
    "    df = df.astype({'covid_count': 'int', 'rnase_count': 'int', 'spikein_count': 'int'}).round({'covid_ratio': 2})\n",
    "    return df\n",
    "\n",
    "def create_pdf(samples, file_name):\n",
    "    fig, ax = plt.subplots(figsize=(35, len(samples) // 2))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    the_table = ax.table(cellText=samples.values, colLabels=samples.columns, loc='center', rowLoc='right',\n",
    "                         bbox=[0, 0, 1, 1])\n",
    "\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(12)\n",
    "\n",
    "    for i in range(0, len(samples.columns)):\n",
    "        [the_table[(j + k, i)].set_facecolor(\"#e0e0e0\") for j in range(4, len(samples), 6) for k in range(3)]\n",
    "\n",
    "    the_table.auto_set_column_width(col=list(range(len(samples.columns))))\n",
    "\n",
    "    pp = PdfPages(f\"/ghds/groups/labdesk/bshih/clinical_observation/{file_name}.pdf\")\n",
    "    pp.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "    pp.close()\n",
    "\n",
    "def slack_upload(fcid, file_name, client):\n",
    "    filepath = f'/ghds/groups/labdesk/bshih/clinical_observation/{file_name}.pdf'\n",
    "\n",
    "    try:\n",
    "        result = client.files_upload(\n",
    "            channels=channel,\n",
    "            file=filepath,\n",
    "            initial_comment=f\"{fcid}: These samples have RNAse Count <= 50 and G19 Score >= 0.01\")\n",
    "        assert result[\"file\"]\n",
    "        print('File Successfully Uploaded to Slack!')\n",
    "    except SlackApiError as e:\n",
    "        # You will get a SlackApiError if \"ok\" is False\n",
    "        assert e.response[\"ok\"] is False\n",
    "        assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "        print(f\"Got an error: {e.response['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = sf_login()\n",
    "client = slack_login()\n",
    "\n",
    "all_samples_singlicate = pd.read_pickle('/ghds/groups/labdesk/bshih/c19dash/c19_dashboard/c19_read_counts.pickle')\n",
    "all_samples = sample_finder(all_samples_singlicate)\n",
    "salesforce_data = salesforce_query(all_samples, sf)\n",
    "blah = merge_sample_salesforce(all_samples, salesforce_data)\n",
    "blah.to_csv('qcfail_samples.csv', index=False)\n",
    "# create_pdf(blah, 'samples_test')\n",
    "# slack_upload('fcid', 'samples_test', client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = slack_login()\n",
    "path = '/ghds/cv19/analysis/210131_NB552482_0012_AHTC5YBGXG.7a84889c-a32d-430d-a19a-e74bd033dc5e.20210131211545/c19_read_counts.hdr.tsv'\n",
    "\n",
    "all_samples_singlicate = pd.read_csv(path, sep='\\t')\n",
    "all_samples = sample_finder(all_samples_singlicate)\n",
    "\n",
    "flowcell = path[20:51]\n",
    "filename = flowcell + '_lowRNAse_COVIDpositive'\n",
    "\n",
    "if len(all_samples) != 0:\n",
    "    sf = sf_login()\n",
    "\n",
    "    salesforce_data = salesforce_query(all_samples, sf)\n",
    "    blah = merge_sample_salesforce(all_samples, salesforce_data)\n",
    "\n",
    "    create_pdf(blah, filename)\n",
    "    slack_upload(flowcell, filename, client)\n",
    "    \n",
    "else:\n",
    "    client.chat_postMessage(channel=channel,\n",
    "                            text=f'{flowcell}: No QC fail samples that are covid positive today! Have a great day :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Testing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcid = '210107_NB552482_0008_AHJ2W2BGXG'\n",
    "\n",
    "pd.read_csv(f'/ghds/cv19/analysis/{fcid}/c19_read_counts.hdr.tsv', sep='\\t').to_csv('/ghds/groups/labdesk/bshih/clinopstest/210103_NB552478_0010_AHJ3KKBGXG_alltrue_test/c19_read_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "LOGGER_NAME = 'clinical_observation'\n",
    "LOG_FILE_DIR = '/ghds/groups/labdesk/bshih'\n",
    "    \n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "log_file = f\"{LOG_FILE_DIR}/clinical_observation.log\"\n",
    "handler = logging.FileHandler(log_file)\n",
    "\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def main():\n",
    "    logger.info(f\"Program is running!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
